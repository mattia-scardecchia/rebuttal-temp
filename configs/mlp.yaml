defaults:
  - /base@_here_
  - _self_

name: mlp
experiment_name: mlp_classifier
seed: 17
N: 100  # data dimension
random_features: false  # if true, freeze all layers except the last
mup: true

# Model parameters
model:
  hidden_dims: [300]  # Hidden layer dimensions
  dropout_rate: 0.0       # Dropout probability
  beta: 1.0 # tanh temperature
  binarize: true  # If true, binarize hidden activations and use straight-through estimator for gradients
  activation: "beta_tanh"  # Options: beta_tanh, square_tanh
  use_bias: false

# Dataloader parameters
dataloader:
  batch_size: 16

# Optimizer parameters
optimizer:
  name: "adamw"  # Options: adam, adamw, sgd
  learning_rate: 0.0003
  weight_decay: 0.001

# Learning rate scheduler
scheduler:
  enabled: false
  name: "plateau"  # Options: step, cosine, plateau
  params:
    factor: 0.5
    patience: 5
    
# Training parameters
trainer:
  max_epochs: 200
  accelerator: "auto"  # Options: cpu, gpu, auto
  devices: 1
  precision: 32  # Options: 32, 16, "bf16"

# Checkpointing parameters
checkpoint_callback:
  enabled: false
  monitor: "val_loss"
  mode: "min"
  save_top_k: 1
  save_last: true
  filename: "epoch={epoch:02d}-val_loss={val_loss:.4f}"
  dirpath: "checkpoints"

# Early stopping parameters
early_stopping:
  enabled: false
  monitor: "train_acc"
  min_delta: 0.001
  patience: 20
  mode: "max"

# Logging parameters
logging:
  dir: "lightning-logs"
  log_every_n_steps: 10

# Hamming balls data
data:
  dataset: mnist
  P: 6000
  P_eval: 1000
  mnist:
    binarize: true
    noise: 0.0
  cifar:
    binarize: true
    cifar10: true
  hm:
    C: 2
    D: 30
    L: 1
    width: 300
    binarize: true
  synthetic:
    P: 30   # Patterns per class (train + val + test)
    C: 10   # Number of classes
    p: 0.3  # Probability of flipping bits from prototypes
    save_dir: "data/balanced_dataset"
    load_if_available: true
    dump: true
    shuffle: true
    val_split: 0.335  # Validation split ratio
    test_split: 0.33  # Test split ratio